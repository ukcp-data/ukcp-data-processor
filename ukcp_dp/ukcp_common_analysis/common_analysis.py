# -*- coding: utf-8 -*-
'''
Common analysis functions for working with data for UKCP18.

Use this module with:

import ukcp_common_analysis.common_analysis as common


--------------------------------------------------------
Contents:

* Six functions for converting ensemble member codes (r001i1p02089 etc)
  between string, dictionary and integer representations:
     ripstr_to_dict(), ripstr_to_int(), 
     ripint_to_str(),  ripint_to_dict(),
     ripdict_to_int(), ripdict_to_str()

* guess_vartag() tries to guess the variable tag (tas, pr etc)
                 from a given cube. Handy as a last resort!

* rectify_units() converts the units of a cube, either according to 
                  an explicit unit provided,
                  or the preferred unit for a variable tag provided.
                  (NOT to be used for temperature changes in Kelvin
                  (e.g. biases/anomalies) - just override the unit!)

* add_mask() takes a cube and adds a mask generated by a given cube of values,
             a comparison operation, and a threshold value.


* add_coord_categories() attempts to add year, season, season_year,
                         month and month_number AuxCoords to the cube provided.


* is_n_monthly() tests to see if the cube's time steps are n-monthly
* is_monthly()   - wrapper to is_n_monthly() for the case of monthly data
* is_seasonal()  - wrapper to is_n_monthly() for the case of seasonal (3-month) data


* get_annlmean_timeseries()    returns a cube of whole-year averages
* get_seasmean_timeseries()    returns a cube of 3-month averages
* get_monthlymean_timeseries() returns a cube of monthly averages
In all of those cases, the necessary categorical AuxCoords are 
added if they're not already present.


* make_climatology() calculates a long-term statistic (mean by default)
                     of a cube, optionally grouping by month or season first.

* make_anomaly() calculates the anomaly (or bias) of one cube with respect to another;
                 Options allow Kelvin temperatures to be properly converted to °C,
                 and for calculating relative anomalies (e.g. %)



* fix_cube_coord_for_pointincell() is a utility function, 
                                   turning 1-d horizontal dimcoords into 2-d auxcoords,
                                   for use with...
* regrid_meanpoints_to_lowres() performs an aggregation type of regridding,
                                where the resulting low-resolution grid cells
                                contain the average of the original high-res
                                grid points that each cell covered.

* regrid_meanpoints_to_lowres_MANUAL() is a slow, explicit, less-general implementation
                                       of the same algorithm.
--------------------------------------------------------


'''
import iris
import iris.coord_categorisation
import cf_units
import numpy as np
import datetime as dt
#=============================================================




#===========================================================================
# Ensemble members are numbered using a system 
# encoding Realisation, Intitialisation and Parameters (RIP).
# In filenames, these are stored like "r001i1p02089",
# whereas in an iris Cube, the realization coord stores them like 1102089
# 
# These functions transform between RIP codes as strings or ints to a dict,
# (and back)
#
def ripstr_to_dict(ripstr):
    '''
    Returns a dictionary with keys 'r','i','p',
    containing the components from the input ripstr, 
    which is a string like "r001i2p34567"
    '''
    firstpass =        ripstr.split("r")  #  ['', '001i2p34567']
    secondpass= firstpass[ 1].split("i")  #  ['001', '2p34567']
    thirdpass = secondpass[1].split("p")  #  ['2', '34567']
    
    rcode = int(secondpass[0])
    icode = int(thirdpass[ 0])
    pcode = int(thirdpass[ 1])
    
    ripdict = dict(r=rcode, i=icode, p=pcode)
    return ripdict


def ripdict_to_int(ripdict):
    '''
    Converts a dictionary with keys r,i,p
    (representing e.g. "r001i2p34567")
    into an integer that looks like 1234567 (note no leading zeros)
    '''
    theint = ripdict["r"]*1000000 + ripdict["i"]*100000 + ripdict["p"]
    return theint

def ripstr_to_int(ripstr):
    '''
    Wrapper of the above two functions,
    converting a string like "r001i2p34567" 
    directly to an integer like 1234567
    '''
    ripdict = ripstr_to_dict(ripstr)
    ripint  = ripdict_to_int(ripdict)
    return ripint



def ripint_to_str(ripint):
    '''
    Convert from an integer like 1234567
    into a string of the form "r001i2p34567"
    '''
    rip_intstr = "{:07d}".format(ripint)
    p = rip_intstr[-5:  ]
    i = rip_intstr[-6:-5]
    r = rip_intstr[ 0:-6]
    # Note we need leading zeroes for r:   
    ripstr = "r{:03d}i{:1s}p{:5s}".format(int(r),i,p)
    return ripstr


def ripint_to_dict(ripint):
    '''
    Wrapper of above functions,
    converting an integer like 1234567
    into a dictionary with keys r,i,p.
    '''
    ripstr = ripint_to_str(ripint)
    ripdict= ripstr_to_dict(ripstr)
    return ripdict


def ripdict_to_str(ripdict):
    '''
    Convert from a dictionary with keys r,i,p
    to a string like "r001i2p34567".
    '''
    ripstr = "r{:03d}i{:1s}p{:5s}".format(ripdict['r'],ripdict['i'],ripdict['p'])
    return ripstr

#===========================================================================





#===========================================================================
def guess_vartag(acube):
    '''
    Try to guess what variable tag is appropriate for a cube
    (e.g. "tas" for near-surface air temperature)

    If the data is set up correctly then it should never be used!
    It has a place currently because this code uses a temporary 
    variable ID system (vartags),
    whereas in future we'll use the variable IDs from the CEDA
    controlled vocabs, which will correspond to the netCDF/cube var_name.
    '''
    VARTAG_CLUES = dict(tas="temperature",ts="temperature",
                        pr="precip", 
                        windspeed="wind" )
    # Go through the keys in VARTAG_CLUES, 
    # testing the cube name for those clues...
    thevartag = None
    for vartag,aclue in VARTAG_CLUES.items():
        if aclue in acube.standard_name:
            thevartag = vartag
            break

    if thevartag is None:
        print "WARNING: Variable tag cannot be identified "
        print "         using standard name '"+acube.standard_name+"',"
        print "         from the list of ",VARTAG_CLUES.keys()

    return thevartag
#===========================================================================




#===========================================================================
def rectify_units(acube, vartag=None, target_unit=None):
    '''
    Correct the units of some standard cases 
    in an automatic fashion.
    
    target_unit is something that can be used by iris.Cube.convert_units(),
    e.g. a string, or a cf_units.Unit object.
    (a good idea might be to provide the preferred_unit component
    of a  ukcp_standard_plots.standards_class.StandardMap object)

    Otherwise, we use vartag (a string, e.g. "tas") 
    to determine what variable we've got,
    and attempt to convert the cube to an assumed preferred unit.

    This will probably be modified in future if it becomes clear that
    the vartag--unit mapping is generally useful.
    '''
    PREFERRED_UNITS = dict(tas = cf_units.Unit("Celsius"),
                           ts  = cf_units.Unit("Celsius"),
                           pr  = cf_units.Unit("mm/day"),
                           mrso= cf_units.Unit("1"),    # soil moisture content in a layer 8223
                           windspeed=cf_units.Unit("m/s") )

    #HANDLED_VARTAGS = PREFERRED_UNITS.keys()
                      
    if target_unit is None:
        if vartag is None:
            # figure out what variable we have...
            vartag = guess_vartag(acube)

        if vartag not in PREFERRED_UNITS.keys():
            print "vartag "+vartag+" found, but target_unit not provided; "
            print "vartag not in list of known preferred units ",PREFERRED_UNITS.keys()
            raise UserWarning("Unable to convert units.")

        # Now we can figure out the preferred unit of this variable:
        target_unit = PREFERRED_UNITS[vartag]
    else:
        # We just need to make sure the target_unit is a Unit object:
        if type(target_unit) is str:
            target_unit = cf_units.Unit(target_unit)



    # Now we know the variable and therefore the target unit,
    # we can apply it.


    # Handle some special cases first:
    WATER_DENSITY = iris.coords.AuxCoord(1000.0,units=cf_units.Unit('kg m-3'))
    ONE_METRE     = iris.coords.AuxCoord(1.0,   units=cf_units.Unit('m')     )
    # (1m = 1 m³/m² for convenience)




    if acube.units.is_convertible("kg m^-2 s^-1") and target_unit.is_convertible("m/s"):
        # WARNING WE'RE ASSUMING IT'S **PRECIPITATION** MASS FLUX
        #         RATHER THAN SOME OTHER kg/m²/s THING - THIS COULD BE RISKY
        
        # Short-circuit a special, common case:
        # (NOT ACTUALLY SURE THIS IS FASTER THAN DOING IT EXPLICITLY...)
        if acube.units == cf_units.Unit("kg m^-2 s^-1"):
            print "Precip mass flux in kg/m²/s  with target compatible with m/s detected;"
            print "Doing short-cut unit conversion..."
            # precip mass flux data in kg/m²/s is numerically equivalent 
            # to precip volume flux data in mm/s.
            # So just override the current units
            # without explicitly dividing by water density.
            acube.units = cf_units.Unit("mm/s")
            # The final call to convert_units will change it from mm/s to mm/day (or whatever)
            
        else:
            print "Precip mass flux detected; dividing by water density to get volume flux..."
            # This can take a while to explcitly do the division,
            # as Biggus will be forced to load the whole cube into memory.
            acube = acube / WATER_DENSITY

        # Tidy up:
        acube.standard_name = "lwe_precipitation_rate"
        acube.long_name     = "precipitation rate"


    
    if acube.units.is_convertible("kg m^-2") and target_unit.is_convertible("1"):
        # WARNING WE'RE ASSUMING IT'S **SOIL MOISTURE** CONTENT
        #         RATHER THAN SOME OTHER kg/m² THING - THIS COULD BE RISKY
        print "Soil moisture content detected, dividing by water areal mass density"
        # soil moisture is given as an areal mass density, kg/m²
        # We need it as a fraction of the areal mass density of water [mass per unit area]
        # (Mass density of Water ρw = 1000 kg/m³, so if we have a m³ of water,
        #  then its mass per unit area is ρw × (1 m³/m²) = 1000×1 kg/m²)
        acube = (acube/ONE_METRE)/WATER_DENSITY
        # (note we can only multiply/deivide Cubes and Coords, NOT Coords and Coords!
        # Tidy up:
        acube.standard_name = "volume_fraction_of_water_in_soil"
        # See http://cfconventions.org/Data/cf-standard-names/43/build/cf-standard-name-table.html
        # I think this is right, but I'm a bit unsure.
        
        # It's a bit clunky though, so specify a long_name too:
        acube.long_name = "soil_moisture_fraction"
        # (RM originally used 'soil_moisture_dimless')
            

    # Hopefully all other cases will "just work":
    acube.convert_units(target_unit)

    return acube
#===========================================================================







#===========================================================================
def add_mask(datacube,maskcube, comparator=">=", threshold=1):
    '''
    Take a cube of data (datacube) and additionally-mask values 
    specified using the maskcube, comparator and threshold.

    e.g. comparator=">" and threshold=1
    means that datacube is (additionally) masked where maskcube > 1
    
    comparator can be a string of ["==","=", ">",">=", "<","<="]
    or any function with signature (x,threshold) where x is an array
    e.g. from the operator package:
    operator.gt, operator.ge, operator.lt, operator.le etc
    (see also http://fcm9/projects/ClimateImpacts/browser/pbett/MyPython/pbutils_comparisons.py?rev=6547)


    So if you have a mask where 1 ==> land, and 0 ==> sea,
    you coud mask out sea points in mycube using 
       mycube = common.add_mask(mycube, mask_cube, comparator="<", threshold=0.5)
    (i.e. all points with mask_cube < 0.5 would be masked out)


    Note that the mask is in addition to any existing masked values,
    but the mask is added to the cube if none is present to start with.
    '''
    # Parse the comparator into a function:
    if type(comparator) is str:
        import operator
        comparators = {"==": operator.eq, 
                       "=" : operator.eq, 
                       ">" : operator.gt,
                       ">=": operator.ge,
                       "<" : operator.lt,
                       "<=": operator.le }
        try:
            comparator_function = comparators[comparator]
        except KeyError:
            raise UserWarning("Comparator string '"+comparator+"' not recognised:\n" + \
                                  "Try one of '==','=', '>','>=', '<','<='.\n" + \
                                  "Failing in common_analysis.py:add_mask()")
    else:
        comparator_function = comparator
            
    # Create the boolean array that will be the new mask:
    themask = comparator_function(maskcube.data, threshold)


    # Copy the cube to add a mask to it:
    # (so the original cube isn't modified)
    datacube_new = datacube.copy()

    # Add the mask to the data cube:
    try:
        _ = datacube.data.mask.shape
        # If that worked, then the cube already has a mask,
        # and we need to just mask some more values:
        datacube_new.data.mask = datacube.data.mask | themask
    except AttributeError:
        # If there was no mask, we need to make a new one using themask array.
        datacube_new.data = np.ma.masked_array( datacube.data, themask )

    # Done!
    return datacube_new
#===========================================================================








#===========================================================================
def add_coord_categories(acube,timecoordname="time", theseasons=('djf','mam','jja','son')):
    '''
    Add time-coord categories.
    This can always be done on-the-fly,
    but it can take a while, 
    so it can often be more efficient to do it once, early on.

    acube is the input cube to add the new AuxCoords to,
    timecoordname is the name of the time coord to categorise,
    and theseasons is an iterable giving a complete list of season names
    (see http://scitools.org.uk/iris/docs/latest/iris/iris/coord_categorisation.html#iris.coord_categorisation.add_season)

    The input cube is modified; nothing is returned.


    We use try/except clauses, so if the coord already exists
    it will (silently) not be added again.
    
    You might want to add *multiple* seasonal coords 
    based on different season definitions (e.g. NDJ,FMA,MJJ,ASO; or JFM,AMJ,JAS,OND)
    this is currently best done manually yourself!!

    ------------------------------------------------------------------------
    WARNING: In  iris 1.13, Lizzie says that there's a bug 
             in the implentation of iris.coord_categorisation.add_day_of_year(),
             in that it doesn't.
             We're not adding day-of-year yet, 
             but bear this in mindif you're thinking about it!
    ------------------------------------------------------------------------
    '''
    try:
        iris.coord_categorisation.add_year(acube,timecoordname, name="year")
    except ValueError:
        pass

    try:
        iris.coord_categorisation.add_season(acube, timecoordname, name='season',
                                             seasons=theseasons )
    except ValueError:
        pass

    try:
        iris.coord_categorisation.add_season_year(acube, timecoordname, name='season_year',
                                                  seasons=theseasons )
    except ValueError:
        pass


    try:
        iris.coord_categorisation.add_month(acube, timecoordname, name='month')
    except ValueError:
        pass      

    try:
        iris.coord_categorisation.add_month_number(acube, timecoordname, name='month_number')
    except ValueError:
        pass

    return 
#===========================================================================










#===========================================================================
def is_n_monthly(acube,nmonths,timecoordname="time"):
    '''
    Test if the time-resolution of acube's time coord
    is nmonths (e.g. monthly for nmonths=1, seasonal for nmonths=3)
    
    This will work for 360-day calendars, 
    and really should work for real-world calendars.
    (but that hasn't been properly tested yet)
    '''
    #----------------------------------------------------
    tcoord=acube.coord(timecoordname)

    # Get the time coord data as a date/datetime/mysterious object:
    # (if it's a 360-day calendar, then it's not a standard python date/datetime,
    #  but a mysterious "instance" object, although it behaves like normal dates;
    #  I think this is a netcdftime.datetime object)
    t = cf_units.num2date( tcoord.points,
                           tcoord.units.name,
                           tcoord.units.calendar)
    t_first = t[0]
    t_next  = t[1]
    
    oldmonth = t_first.month

    new_day   = t_first.day
    new_month = ((oldmonth+nmonths)-1) % 12 +1
    new_year  = t_first.year+( ((oldmonth+nmonths)-1) /12 )

    # Create a new object using the class constructor
    # of t_first's class 
    # (could be a datetime.date, datetime.datetime, netcdftime.datetime, ...)
    newdate = t_first.__class__(new_year, new_month, new_day)

    # With netcdftime.datetime objects, you can't change the components:
    #newdate       = copy.copy(t_first)
    #newdate.month = ((oldmonth+nmonths)-1) % 12 +1
    #newdate.year  = t_first.year+( ((oldmonth+nmonths)-1) /12 )
    #newdate.day  = t_first.day    (don't need to change this)
    
    # After adding n months, is the result the same as the next date?
    is_nmonthly = t_next == newdate

    return is_nmonthly



def is_monthly(acube,timecoordname="time"):
    '''
    Convenience wrapper to is_n_monthly() for n=1,
    i.e. testing if acube contains monthly data.
    '''
    return is_n_monthly(acube,1,timecoordname=timecoordname)


def is_seasonal(acube,timecoordname="time"):
    '''
    Convenience wrapper to is_n_monthly() for n=3,
    i.e. testing if acube contains seasonal data.
    '''
    return is_n_monthly(acube,3,timecoordname=timecoordname)


#===========================================================================


















#===========================================================================
def get_annlmean_timeseries(acube, timecoordname="time"):
    '''
    Handy function to get annual means each year.
    Note that a 'year' categorical AuxCoord will be added if necessary.
    '''        
    try:
        iris.coord_categorisation.add_year(acube,timecoordname, name="year")
    except ValueError:
        pass
    annlmeans = acube.aggregated_by('year',operation)
    return annlmeans



def get_seasmean_timeseries(acube, timecoordname="time",theseasons=('djf','mam','jja','son')):
    '''
    Handy function to get seasonal means each year.
    Note that 'season' and 'season_year' categorical AuxCoords will be added if necessary.

    theseasons is an iterable giving a complete list of season names
    (see http://scitools.org.uk/iris/docs/latest/iris/iris/coord_categorisation.html#iris.coord_categorisation.add_season)
    '''        
    try:
        iris.coord_categorisation.add_season(acube, timecoordname, name='season',
                                             seasons=theseasons )
    except ValueError:
        pass

    try:
        iris.coord_categorisation.add_season_year(acube, timecoordname, name='season_year',
                                                  seasons=theseasons )
    except ValueError:
        pass

    seascube = acube.aggregated_by(["season","season_year"],iris.analysis.MEAN)
    return seascube


    

def get_monthlymean_timeseries(acube, timecoordname="time"):
    '''
    Handy function to get monthly means each year.

    Note that 'year' and 'month_number' categorical AuxCoords will be added if necessary.
    '''    
    try:
        iris.coord_categorisation.add_year(acube,timecoordname, name="year")
    except ValueError:
        pass

    try:
        iris.coord_categorisation.add_month_number(acube, timecoordname, name='month_number')
    except ValueError:
        pass

    moncube = acube.aggregated_by(['month_number','year'],iris.analysis.MEAN)
    return moncube
#===========================================================================






#===========================================================================
def make_climatology(acube, timecoordname='time',year_range=None, 
                     seasonyear_range = None,
                     climtype="annual", operation=iris.analysis.MEAN):
    '''
    Make a "climatology", i.e. a statistic calculated over a long period of time.

    The obvious example is the long-term mean over many years,
    but you can provide any iris.analysis function as the "operation" argument,
    http://scitools.org.uk/iris/docs/latest/iris/iris/analysis.html
    
    You can also do monthly or seasonal climatologies:
    climtype ("climatology type")  must be one of 'annual','seasonal','monthly'.
    (acutally, just starting with seas* or month* will do)

    Seasons and month categorical AuxCoords will be added if necessary
    (so if you want special season definitions, e.g. NDJ instead of DJF,
     you should add them yourself first!)
     
    In the case of seasonal/monthly climatologies, 
    the resulting cube has an anonymous leading dimension (DimCoord),
    which is linked to time, year, and season or month & month_number AuxCoords.
     
    The climatology will be calculated over all timesteps in acube by default.
    Alternatively, you can provide a 2-element tuple/list to year_range,
    specifying the subset of years to include;
    an intermediate cube will be extracted covering that range.
    Note that the years selected will go from year_range[0] to year_range[1] INCLUSIVE.

    Alternatively, you can do the same thing to a season_year coordinate,
    by giving that 2-element tuple/list to the seasonyear_range argument instead.
    This means that Decembers will be kept with the subsequent year
    (e.g. Dec 1980 is in season_year 1981),
    which is what we usually want to do for UKCP18...

    '''

    if year_range is not None:
        try:
            iris.coord_categorisation.add_year(acube,timecoordname, name="year")
        except ValueError:
            pass
        # Now extract the subset of years:
        year_constraint = iris.Constraint(year=lambda cell: \
                                              year_range[0] <= cell <= year_range[1] )
        thiscube = acube.extract(year_constraint)
    else:
        thiscube = acube


    if seasonyear_range is not None:
        try:
            iris.coord_categorisation.add_season_year(acube,timecoordname, name="season_year")
        except ValueError:
            pass
        # Now extract the subset of years:
        seasyear_constraint = iris.Constraint(season_year=lambda cell: \
                                                  seasonyear_range[0] <= cell <= seasonyear_range[1] )
        thiscube = acube.extract(seasyear_constraint)
    else:
        thiscube = acube


    
    climtype = climtype.lower()     
    if climtype.startswith("seas"):
        theseasons = ('djf', 'mam', 'jja', 'son')
        try:
            iris.coord_categorisation.add_season(thiscube, timecoordname, name='season',
                                                 seasons=theseasons )
        except ValueError:
            pass

        # Because the mean of a mean is a mean, there isn't an issue if we've requested means.
        # But if we've requested standard deviations for example, 
        # we need to make sure we start with a seasonal-mean time series.
        if operation is not iris.analysis.MEAN:
            # The more general case: get the seasonal means first,
            # then perform the requested interannual aggregation for the seasonal means.
            if not is_seasonal(thiscube,timecoordname=timecoordname):
                print "Calculating seasonal means prior to applying "+operation.name()+" interannually..."
                seascube = get_seasmean_timeseries(thiscube, timecoordname=timecoordname,
                                                   theseasons=theseasons)
            else:
                #print "   (it's ok, data is already seasonal)"
                seascube = thiscube
        else:
            seascube = thiscube

        # Now we are safe to aggregate:
        climatol = seascube.aggregated_by('season',operation)
        # This is likely to result in an anonymous dimension
        # with time, season and season_year (and possibly other) aux coords.

        # It might be nice to have an option to automatically remove these,
        # and promote the season aux coord to the dim coord.
        # Promoting is easy
        # (http://scitools.org.uk/iris/docs/v1.9.0/html/iris/iris/util.html#iris.util.promote_aux_coord_to_dim_coord)
        # but it's probably a pain to get the list of other aux coords in that dimension
        # and remove them in a loop.
        # So, the user will have to do this themselves if required.


    elif climtype.startswith("month"):
        try:
            iris.coord_categorisation.add_month(       thiscube, timecoordname, name='month')
        except ValueError:
            pass
        try:
            iris.coord_categorisation.add_month_number(thiscube, timecoordname, name='month_number')
        except ValueError:
            pass
        
        # Because the mean of a mean is a mean, there isn't an issue if we've requested means.
        # But if we've requested standard deviations for example,
        # we need to make sure we start with a monthly-mean time series.
        if operation is not iris.analysis.MEAN:
            # The more general case: get the monthly means first if necessary,
            # then perform the requested interannual aggregation for the monthly means.
            if not is_monthly(thiscube,timecoordname=timecoordname):
                print "Calculating monthly means prior to applying "+operation.name()+" interannually..."
                moncube = get_monthlymean_timeseries(thiscube, timecoordname=timecoordname)
            else:
                print "   (it's ok, data is already monthly)"
                moncube = thiscube
        else:
            moncube = thiscube

        # Now we are safe to aggregate:
        climatol = moncube.aggregated_by('month_number',operation)
        # As with seasonal, this will result in an anonymous dim coord
        # covering time, month, month_number etc,
        # but in this case we'd want to retain BOTH month and month_number.
        # So it's even less clear how to automaticaly tidy in this case.



    elif climtype=="annual":
        # Because the mean of a mean is a mean, there isn't an issue if we've requested means.
        # But if we've requested standard deviations for example, 
        # we need to make sure we start with an annual-mean time series.
        if operation is not iris.analysis.MEAN:
            # The more general case: get the annual means first if necessary,
            # then perform the requested interannual aggregation for the annual means.
            annlcube = get_annlmean_timeseries(thiscube, timecoordname=timecoordname)
        else:
            annlcube = thiscube
        
        # Now we are safe to aggregate -- actually, we want to COLLAPSE
        # over all time in this case!
        climatol = annlcube.collapsed('time',operation)

        
    else:
        raise UserWarning("Climate type ("+climtype+") not recognised! \n"+
                          "Use annual, sesonal or monthly.")



    return climatol
#===========================================================================











#===========================================================================
def make_anomaly(datacube,reference_cube,preferred_unit=None, name_tag="anomaly"):
    '''
    Calculate the anomaly (or bias) of acube with respect to a reference_cube.
    These must be compatible shapes, or it will fail.

    Not only that: they also can't have AuxCoords that differ in values
    (e.g. when using seasonal data, the time coord gets smooshed into an 
     anonymous dimcoord along with things like season, season_year, 
     forecast_period...
     If the data cover different time periods, then you'll need to do 
     cube.remove_coord for time,season_year and forecast_period so that 
     you've just got the season coord left)

    We might want to add some automatic processing to try 
    to smooth over these sort of issues automatically in future...

    
    If the user sets the preferred_unit to be dimensionless
    (i.e.  preferred_unit.is_convertable(1) )
    then we take that to imply a request for RELATIVE anomalies,
    i.e. (x-μ)/μ instead of just x-μ   (where μ is the mean of x)
    This might be the case for precip, where we want anomalies in %.


    The name_tag will be appended to the existing long/standard name
    in the long_name of the new cube.
    You probably want it to be "anomaly" or "bias".
    '''
    # Actually calculating the anomalies is trivial:
    anomaly = datacube - reference_cube


    #-------------------------------------------------------------
    if preferred_unit is not None:
        if preferred_unit == cf_units.Unit("Celsius"):
            # Manually switch temperature anomaly units to °C, 
            # to avoid errors later:
            # ΔT in K is the same as ΔT in °C, 
            # so subtracting 273.15K when converting units would be very wrong!
            if anomaly.units == cf_units.Unit("Kelvin"):
                anomaly.units = cf_units.Unit("Celsius")
        
        if preferred_unit.is_convertible(1):
            # We've asked for RELATIVE anomalies,
            # i.e. (x-μ)/μ instead of just x-μ (where μ is the mean of x)
            # This might be the case for precip where we want anomalies in %.
            anomaly = anomaly/reference_cube
        
        # Now apply the unit change:
        anomaly.convert_units(preferred_unit)
    #-------------------------------------------------------------
        
    #-------------------------------------------------------------
    # The cube's names have been lost.
    # We copy over the standard name, and make a new long name.
    anomaly.standard_name = datacube.standard_name
    if datacube.long_name is not None:
        anomaly.long_name = datacube.long_name
    else:
        anomaly.long_name = datacube.standard_name

    # Append the name_tag to the long_name
    # (we'll do something different later probably!)
    anomaly.long_name += " " + name_tag

    # We will want to include a new attribute 'climatology_baselne'
    # that describes the dates of the climatology in the reference_cube,
    # as a string like "yyyy-mm-dd yyyy-mm-dd"
    # Ideally we could just take this from the reference_cube,
    # so it should be made by a function called by make_climatology()...
    
    
    # We DON'T copy across the other attributes, 
    # because we could be mixing data from different models/obs here.

    # We MIGHT want to copy across the cell_methods from the
    # original datacube though.
    #-------------------------------------------------------------



    return anomaly
#===========================================================================












#=============================================================================
def fix_cube_coord_for_pointincell(acube, grid_y_name, grid_x_name): 
    '''
    Phil's version of a function originally by Ségolène Berthou,
    [ ~sberthou/workspace/EXT_PRC_PLT/REGRID/regridding.py ]
    which prepares a cube for regridding 
    by iris.experimental.regrid.PointInCell().

    grid_y_name and grid_x_name are the names of the DimCoords
    for the horizontal spatial dimensions of the cube,
    and must be 1-dimensional (we'll be making them 2-d here).
    They're likely to not be purely lat/lon, but on
    some sort of projected grid.
   
    What we need to do is broadcast up the cube's Y and X coords
    (which might be called grid_longitude or projection_x_coordinate
     or something)
    to be 2-dimensional, 
    and remove any actual lat/lon coords if present.

    The resulting cube will have anonymous X & Y dims,
    with 2-dimensional AuxCoords associated with both dims.

    The input cube can still have other dimensions (e.g. time).
    
    This function can then be followed by a call to 
    cube.regrid( somecube, iris.experimental.regrid.PointInCell() )

    Note that this makes a copy of the input cube,
    and returns the modified cube.
    '''
    newcube = acube.copy()
    try:
        assert newcube.coord(grid_y_name).ndim == 1
    except AssertionError:
        raise UserWarning("Y-coordinate provided wasn't 1-dimensional!")
    try:
        assert newcube.coord(grid_x_name).ndim == 1
    except AssertionError:
        raise UserWarning("X-coordinate provided wasn't 1-dimensional!")

    FINAL_YCOORDNAME = 'projection_y_coordinate'
    FINAL_XCOORDNAME = 'projection_x_coordinate'
    FINAL_YCOORDUNITS= newcube.coord(grid_y_name).units # 'degrees' or 'm'
    FINAL_XCOORDUNITS= newcube.coord(grid_x_name).units # probably == FINAL_YCOORDUNITS

    # This is needed for mapping the final 2-d coords back onto the cube:
    YX_DIMS = (newcube.coord_dims(grid_y_name)[0], 
               newcube.coord_dims(grid_x_name)[0])
    
    YX_SHAPE = (newcube.coord(grid_y_name).points.size, 
                newcube.coord(grid_x_name).points.size)
    # If the cube dims are like (t,y,x), then this is like (len(y),len(x));
    # but it also works if there're no other dims, 
    # so it's a bit more general than just saying newcube.shape[1:]
    # It also lets us impose the order of the resulting 
    # spatial dims:  Y then X (as usual, e.g. lat then lon)
    # This is used in the last argument of broadcast_to_shape below...
    

    #---------------------------------------------------------
    ycoord   = newcube.coord(grid_y_name)
    # Tile the y points to have the same shape as the spatial part of the cube:
    # (Note the dim mapping of just [0], as the y-coord is dim 0 in YX_SHAPE)
    ypoints  = iris.util.broadcast_to_shape(ycoord.points, YX_SHAPE, [0])
    ycoord2d = iris.coords.AuxCoord(ypoints, FINAL_YCOORDNAME, 
                                    units=FINAL_YCOORDUNITS, 
                                    coord_system=ycoord.coord_system)
    #---------------------------------------------------------    


    #---------------------------------------------------------    
    xcoord   = newcube.coord(grid_x_name)
    # Tile the x points to have the same shape as the spatial part of the cube:
    # (Note the dim mapping of just [1], as the y-coord is dim 1 in YX_SHAPE)
    xpoints  = iris.util.broadcast_to_shape(xcoord.points, YX_SHAPE, [1])
    xcoord2d = iris.coords.AuxCoord(xpoints, FINAL_XCOORDNAME,
                                    units=FINAL_XCOORDUNITS, 
                                    coord_system=xcoord.coord_system)
    #---------------------------------------------------------


    #---------------------------------------------------------
    newcube.remove_coord(grid_y_name)
    newcube.remove_coord(grid_x_name)

    newcube.add_aux_coord(ycoord2d, YX_DIMS)
    newcube.add_aux_coord(xcoord2d, YX_DIMS)
    #---------------------------------------------------------    


    #---------------------------------------------------------
    # Finally, remove any extraneous lat/lon coords
    # (if they weren't the dim coords we were just playing with)
    if grid_y_name != "latitude":
        try: 
            newcube.remove_coord('latitude')
        except iris.exceptions.CoordinateNotFoundError:
            pass
    
    if grid_x_name != "longitude":
        try: 
            newcube.remove_coord('longitude')
        except iris.exceptions.CoordinateNotFoundError:
            pass
    #---------------------------------------------------------
    return newcube







def regrid_meanpoints_to_lowres(smlcube,bigcube, mask=None,
                                grid_y_name='projection_y_coordinate',
                                grid_x_name='projection_x_coordinate', 
                                verbose=False):
    '''
    Perform a kind of regridding/aggregating 
    between two 2-dimensional cubes:
    
    smlcube is POINT data, on an equal-area grid
    (potentially high-resolution gridded obs)
    (equal area because we're NOT weighting here - although we could...)

    bigcube is GRID-CELL data
    (representing the average value over each cell region)
    with latitude and longitude coords
    (potentially lower-resolution model data).

    We use the iris.experimental.regrid.PointInCell() scheme
    http://scitools.org.uk/iris/docs/latest/iris/iris/experimental/regrid.html?highlight=pointincell#iris.experimental.regrid.PointInCell
    to take the average of all the smlcube points within each bigcube cell,
    returning the resulting newcube (at the bigcube resoluion).

    Optionally, an additional mask is applied through a logical OR
    with any mask resulting from the regridding operation.
    
    
    Most of the work is actually done in fix_cube_coord_for_pointincell() 
    above, which turns the input horizontal 1-d dimcoords into 2-d auxcoords,
    and removes the lat/lon coords if necessary.
    '''
    try:
        assert bigcube.ndim == 2
    except AssertionError:
        raise UserWarning("bigcube is not 2-dimensional!")
    # (Other tests are performed in fix_cube_coord_for_pointincell)

    # Rearrange the spatial coordinates:
    if verbose: print "Rearranging smlcube..."
    smlcube_fixed = fix_cube_coord_for_pointincell(smlcube, grid_y_name, grid_x_name)
    if verbose: print "Rearranging of smlcube done, resulting cube is:\n"+smlcube_fixed.__repr__()


    # Do the regridding:
    if verbose: print "Performing the regridding by iris.experimental.regrid.PointInCell..."
    newcube = smlcube_fixed.regrid(bigcube, iris.experimental.regrid.PointInCell())
    if verbose: print "Regridding done, resulting cube is:\n"+newcube.__repr__()


    # If an additional mask has been provided, apply it also:
    if mask is not None:
        if verbose: print "Applying additional mask..."
        newcube.data.mask = newcube.data.mask | mask

    return newcube
#============================================================================




#============================================================================
def regrid_meanpoints_to_lowres_MANUAL(smlcube,bigcube, mask=None, 
                                       guess_bounds=True):
    '''
    Perform a kind of regridding/aggregating 
    between two 2-dimensional cubes:
    
    smlcube is POINT data, with latitude and longitude coords
    (potentially high-resolution gridded obs)

    bigcube is GRID-CELL data
    (representing the average value over each cell region)
    with latitude and longitude coords
    (potentially lower-resolution model data).

    We find the set of points from smlcube 
    that lie within each bigcube grid cell,
    and average them in each cell of a new cube (at the bigcube resoluion).

    Optionally, an additional mask is applied through a logical OR
    with any mask resulting from the regridding operation.

    As the smlcube's coords and data arrays are both flattened,
    this will handle 2-dimensional coordinates fine.

    Because the nearest-neighbour routine will behave differently if the bigcube
    has bounds or not, this function guesses them if they're not present by default
    (like the reg_extract() function in regions.py)

    I might want to extend this to allow smlcube to have more dimensions,
    e.g. time!!

    Another method would be to set up a k-dimensional tree 
    to make the nearest-neighbour calculations much more efficient.
    SciPy implements this as 
    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html
    but I simply haven't tested it yet.
    The difficulites in making sure we've unwrapped the dimensions correctly 
    would remain.

    ********************************************************
    *** THIS IS REPLACABLE BY THE PointInCell REGRIDDER  ***
    *** WHICH IS FASTER AND MORE GENERAL!                ***
    *** SEE regrid_meanpoints_to_lowres() FUNCTION ABOVE ***
    ********************************************************
    '''
    #---------------------------------------------------------------------------
    # We're quite restrictive on what we'll accept here.
    # Both cubes must be 2-dimensional, and have lat & lon coords.
    try:
        assert smlcube.ndim == 2
    except AssertionError:
        raise UserWarning("smlcube is not 2-dimensional!")
    try:
        assert bigcube.ndim == 2
    except AssertionError:
        raise UserWarning("bigcube is not 2-dimensional!")

    try:
        assert bigcube.coord_dims('latitude')  == (0,)
        assert bigcube.coord_dims('longitude') == (1,)
    except AssertionError:
        raise UserWarning("bigcube latitude and longitude must be dims 0 and 1!")

    try:
        nlatdim = smlcube.coord('latitude').ndim
    except iris.exceptions.CoordinateNotFoundError:
        raise UserWarning("Latitude coord not present in smlcube!")
    try:
        nlondim = smlcube.coord('longitude').ndim
    except iris.exceptions.CoordinateNotFoundError:
        raise UserWarning("Longitude coord not present in smlcube!")

    coordnames = ['latitude','longitude']
    for coordname in coordnames:
        if not bigcube.coord(coordname).has_bounds():
            if guess_bounds:
                print "Note: bigcube didn't have "+coordname+" bounds, guessing them..."
                bigcube.coord(coordname).guess_bounds()
            else:
                print "WARNING: bigcube didn't have "+coordname+" bounds!"
                print"          Regridding of smlcube to bigcube MIGHT be different than if they'd been there!"
    #---------------------------------------------------------------------------

    # Set the order for flattening the smlcube arrays:
    # (flatten always copies, ravel only does it if necessary)
    #ORDER="K"   #  Memory order, apart from reversed axes
    #ORDER="A"   #  Memory order for Fortran (col-major) or C (row-major) ordering
    #ORDER="F"   #  Fortran-style (col-major) order
    ORDER ="C"   #  C-style (row-major) order (rightmost index varies fastest) - default
    #               (A different choice would mean switching the np.tile/np.repeat lines
    #                for the 1-dimensional lat/lon case below 
    #                -- these are currently written EXCPLICITLY for C-style row major arrays.)
    
    biglon = bigcube.coord('longitude')
    biglat = bigcube.coord('latitude' )
    smllon = smlcube.coord('longitude')
    smllat = smlcube.coord('latitude' )


    # Get arrays of the nearest bigcube grid cell 
    # to each of the smlcube's points.
    print "Finding which grid cells contain which points..."

    # If the coords are 2-d already, then flattening them is all we need to do.
    # If they are 1-d, then we need to repeat them appropriately,
    # i.e. according to the order we'll use for flattening the data (ORDER):
    if nlondim == 2:
        smllonpoints = smllon.points.ravel(ORDER)
    elif nlondim == 1:
        smllonpoints = np.tile( smllon.points.ravel(ORDER), len(smllat.points) ) # a,b,c,a,b,c,a,b,c,...
    else:
        raise UserWarning("Longitude of smlcube neither 2 nor 1-dimensional!")

    if nlatdim == 2:
        smllatpoints = smllat.points.ravel(ORDER)
    elif nlatdim == 1:
        smllatpoints = np.tile( smllat.points.ravel(ORDER), len(smllat.points) ) # a,b,c,a,b,c,a,b,c,...
    else:
        raise UserWarning("Latitude of smlcube neither 2 nor 1-dimensional!")


    # http://scitools.org.uk/iris/docs/latest/iris/iris/coords.html?highlight=nearest#iris.coords.Coord.nearest_neighbour_index
    # Notes:
    #   * this only works with 1-D coordinates
    #   * if coord bounds are present, they'll be used instead of the coord points  
    nearest_biglon_indices = np.array([biglon.nearest_neighbour_index(p) for p in smllonpoints])
    nearest_biglat_indices = np.array([biglat.nearest_neighbour_index(p) for p in smllatpoints])
    



    
    # Create new cube,
    # for storing the mean of the high-res data on the low-res grid:
    newcube = bigcube.copy()
    newcube.data = np.full(bigcube.data.shape, np.nan)
    newcube.long_name     = smlcube.long_name
    newcube.standard_name = smlcube.standard_name
    newcube.var_name      = smlcube.var_name
    newcube.units         = smlcube.units
    newcube.attributes    = smlcube.attributes

    # For each bigcube grid cell, 
    # pick out smlcube data points where this bigcube cell is the nearest,
    # and place the average of those points in the newcube's grid cell:
    print "Calculating means of points in each grid cell..."
    smldata = smlcube.data.ravel(ORDER)
    for ilat in range(biglat.points.size):
        #print "ilat=",ilat,"/",biglat.points.size-1
        for ilon in range(biglon.points.size):
            # Note that the brackets are important to get the logic order right here:
            isml_in_big = np.where( (nearest_biglon_indices==ilon) & (nearest_biglat_indices==ilat) )[0]
            if isml_in_big.size > 0:
                newcube.data[ilat,ilon] = np.mean(smldata[isml_in_big])

    # Any remaining NaNs should be masked:
    newcube.data = np.ma.masked_invalid(newcube.data)

    #import code ;  code.interact(local=locals())


    # If an additional mask has been provided, apply it also:
    if mask is not None:
        newcube.data.mask = newcube.data.mask | mask

    return newcube
#=============================================================================















#==============================================================================
if __name__=="__main__":
    print "====================================================================================="
    print "You have tried to run   ukcp_common_analysis.common_analysis   from the command line."
    print "This does nothing. Try importing the module and using its functions in your own work!"
    print "====================================================================================="
#=========================================================================

